# -*- coding: utf-8 -*-
"""predicting_the_customer_lifetime_value.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h03yuVHln5oNbLxvmbE2O_lfhWpap849

Problem Statement


VahanBima is one of the leading insurance companies in India. It provides motor vehicle insurances at best prices with 24/7 claim settlement.  It offers different types of policies for  both personal and commercial vehicles. It has established its brand across different regions in India. 

Around 90% of the businesses today use personalized services. The company wants to launch different personalized experience programs for customers of VahanBima. The personalized experience can be dedicated resources for claim settlement, different kinds of services at doorstep, etc. Inorder to do so, they would like to segment the customers into different tiers based on their customer lifetime value (CLTV).
"""

#Importing Pandas and numpy Libraries 
import pandas as pd 
import numpy as np

#mounting the drive
from google.colab import drive
drive.mount("/content/drive")

# reading all the files from google drive
train = pd.read_csv("/content/drive/My Drive/analytics vidya/train_data.csv")
test  = pd.read_csv("/content/drive/My Drive/analytics vidya/test_data.csv")

#printing no.of Rows and columns
print(train.shape)
print(test.shape)

#checking the top 5 records of train data
train.head()

#checking the top 5 records of train data
test.head()

#information of train data
train.info()

#information of test data
test.info()

#cheching the unique values for train
train.nunique()

#cheching the unique values for test
test.nunique()

#checking the Data types of train data
train.dtypes

#checking the Data types of test data
test.dtypes

#summary of numerical columns of train data
train.describe()

#summary of numerical columns of test data
test.describe()

# Converting Categorical columns to category data type
cat_cols = ['gender','area','qualification','income','marital_status','vintage','num_policies','policy','type_of_policy']
train[cat_cols]=train[cat_cols].astype('category')
test[cat_cols]=test[cat_cols].astype('category')

#cheching the data types of train
train.dtypes

#cheching the data types of test
test.dtypes

#Checking for null values in the train 
print(train.isnull().sum())

#Checking for null values in the test
print(test.isnull().sum())

"""Exploratory Data Analysis (EDA)"""

#importing seaborn and matplotlib for visualization 
import seaborn as sns 
import matplotlib.pyplot as plt

#Barplot of gender wise income
sns.barplot(train["gender"],train["cltv"]).set_title("gender Wise income")

sns.barplot(train["gender"],train["claim_amount"]).set_title('claim amount of genders')

#From the below Heatmap we can clearly states that claim_amount and cltv are highly correlated
plt.figure(figsize=(13,10))
sns.heatmap(train.corr(),annot=True,cmap="Accent",cbar=True)

# Dropping the Target column cltv
X=train.drop(["cltv"],axis =1)
y=train["cltv"]

#importing train_test_split from sklearn.model_selection and spliting the train_df into train and validation data 
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=123)

#Shape of all the datset after spliting the data 
print(X_train.shape)
print(X_val.shape)
print(test.shape)
print(y_train.shape)
print(y_val.shape)
print(test.shape)

#importing OneHotEncoder from sklearn and fitting OneHotEncoder for X_train
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(drop = 'first',handle_unknown="ignore")
encoder.fit(X_train[cat_cols])

# Transforming  OneHotEncoder on both train_df and test_df
train_ohe=encoder.transform(X_train[cat_cols]).toarray()
val_ohe=encoder.transform(X_val[cat_cols]).toarray()
test_ohe=encoder.transform(test[cat_cols]).toarray()

#Assigning numerical columns to num_cols 
num_cols = ["claim_amount"]

#importing  StandardScaler from sklearn.preprocesing fitting StandardScaler for X_train
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train[num_cols])

# Transforming  standardscaler on both train and test
train_std = scaler.transform(X_train[num_cols])
val_std = scaler.transform(X_val[num_cols])
test_std = scaler.transform(test[num_cols])

#Concatenating all the numerical and categorical columns after standardisation & OneHotEncoding 
X_train= np.concatenate([train_ohe,train_std], axis=1)
X_val= np.concatenate([val_ohe,val_std], axis=1)
X_test = np.concatenate([test_ohe,test_std], axis=1)

"""MODEL BUILDING

Decision Tree Regressor
"""

#importing DecisionTreeRegressor ,error metrics and export_graphviz from sklearn.tree.metrics and fitting it to X_train and y_train
from sklearn.tree import  DecisionTreeRegressor,export_graphviz
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import math
reg_dt = DecisionTreeRegressor()
reg_dt.fit(X_train,y_train)

train_pred = reg_dt.predict(X_train)
val_pred = reg_dt.predict(X_val)

print("R2 score",r2_score(y_pred=train_pred,y_true=y_train))
print("R2 score",r2_score(y_pred=val_pred,y_true=y_val))

# Setting the parameter to test
param_grid = {"min_samples_split": [0,5],
              "max_depth": [None, 6],
              "min_samples_leaf": [1, 8]
             }

#importing GridSearchCV from model_selection 
from sklearn.model_selection import GridSearchCV
reg_dt_grid = GridSearchCV(reg_dt,param_grid,cv=6)

#Fitting the reg_dt_grid to X_train & y_train
reg_dt_grid.fit(X_train,y_train)

reg_dt_grid.best_params_

train_pred = reg_dt_grid.predict(X_train)
val_pred= reg_dt_grid.predict(X_val)

print("R2 score",r2_score(y_pred=train_pred,y_true=y_train))
print("R2 score",r2_score(y_pred=val_pred,y_true=y_val))

# Setting the parameter to test
param_grid = {"min_samples_split": [0,10],
              "max_depth": [None, 8],
              "min_samples_leaf": [1, 15]
             }

reg_dt_grid2 = GridSearchCV(reg_dt,param_grid,cv=10)

#Fitting the reg_dt_grid2to X_train & y_train
reg_dt_grid2.fit(X_train,y_train)

reg_dt_grid2.best_params_

train_pred = reg_dt_grid2.predict(X_train)
val_pred = reg_dt_grid2.predict(X_val)

print("R2 score on test data",r2_score(y_pred=train_pred,y_true=y_train))
print("R2 score on val data",r2_score(y_pred=val_pred,y_true=y_val))

"""Random Forest"""

#importing RandomForestRegressor from sklearn.ensemble  and fitting it to X_train,y_train
from sklearn.ensemble import RandomForestRegressor
reg_rf = RandomForestRegressor()
reg_rf.fit(X_train, y_train)

train_pred = reg_rf.predict(X_train)
val_pred = reg_rf.predict(X_val)

print("R2 score on test data",r2_score(y_pred=train_pred,y_true=y_train))
print("R2 score on val data",r2_score(y_pred=val_pred,y_true=y_val))

"""Hyper Parameter Tuning for random Forest"""

reg_rf1 = RandomForestRegressor()
reg_rf1.fit(X_train, y_train)

param_grid = {"n_estimators" : [10, 80],
              "max_depth" : [1,10],
              "max_features" : [2, 9],
              "min_samples_leaf" : [2, 7, 10]}

reg_rf1_grid = GridSearchCV(reg_rf1,param_grid, cv=5)

reg_rf1_grid.fit(X_train,y_train)

reg_rf1_grid.best_params_

train_pred = reg_rf1_grid.predict(X_train)
val_pred = reg_rf1_grid.predict(X_val)

print("R2 score on test data",r2_score(y_pred=train_pred,y_true=y_train))
print("R2 score on val data",r2_score(y_pred=val_pred,y_true=y_val))

train_pred = reg_rf.predict(X_train)
val_pred = reg_rf.predict(X_val)
test_pred = reg_rf.predict(X_test)

"""XG Boost"""

!pip install xgboost

from xgboost import XGBRegressor

xgb=XGBRegressor(n_estimators=1000,max_depth=7,eta=0.1,subsamples=0.7,colsample_bytree=0.8)

xgb.fit(X_train,y_train)

train_pred=xgb.predict(X_train)
val_pred=xgb.predict(X_val)

print("R2 score on test data",r2_score(y_pred=train_pred,y_true=y_train))
print("R2 score on val data",r2_score(y_pred=val_pred,y_true=y_val))

#load train and test files
import pandas as pd
train = pd.read_csv("/content/drive/My Drive/analytics vidya/train_data.csv")
test  = pd.read_csv("/content/drive/My Drive/analytics vidya/test_data.csv")

#taking the mean of cltv from the training set
test['cltv']=train['cltv'].mean()

#creating the sample submission file
sample_submission = test[['id','cltv']]
sample_submission.to_csv("sample_submission.csv",index=False)

"""From the above models we can clearly states that Random Forest Regreesor is giving the highest R-squared value of 0.82353 on test data"""